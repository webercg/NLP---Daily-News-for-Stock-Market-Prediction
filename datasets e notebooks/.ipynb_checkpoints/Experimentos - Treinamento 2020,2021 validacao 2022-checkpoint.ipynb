{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/webercg/NLP---Daily-News-for-Stock-Market-Prediction/blob/main/Experimentos_%2B_EDA_(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aMeKxD1txAj"
   },
   "source": [
    "# 1.0 Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PN29buKktxAm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP ZBook 15\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualização de dados\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Manipulação datas\n",
    "from datetime import datetime\n",
    "\n",
    "# Prototipação\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "\n",
    "#Pipeline e pré-process\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "#Model Tunning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Model evaluation\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Gerando dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando dataframe e gerando features Sentilex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Var%</th>\n",
       "      <th>Fechamento</th>\n",
       "      <th>Noticias</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>20.47</td>\n",
       "      <td>37774500</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>petrobras e vale retiram seus funcionários de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>20.30</td>\n",
       "      <td>71595600</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>petrobras reduz preços do diesel e da gasolin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>20.54</td>\n",
       "      <td>81844000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>petrobras faz redução em produção de petróle...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>20.46</td>\n",
       "      <td>32822000</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>o adeus da petrobras ao amazonas petrobras vê...</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>20.27</td>\n",
       "      <td>36102700</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>refinarias da petrobras apresentam queda na c...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>30.60</td>\n",
       "      <td>118295600</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0</td>\n",
       "      <td>bolsonaro defende direito de mudar ceo da pet...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2022-05-30</td>\n",
       "      <td>29.99</td>\n",
       "      <td>98110100</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>lira sugere que governo venda ações da petro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>30.06</td>\n",
       "      <td>83598000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>bolsonaro diz que quer fatiar a petrobras bol...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>30.02</td>\n",
       "      <td>46353200</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>petrobras diz que oferta de ações da braskem ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>2022-06-06</td>\n",
       "      <td>30.30</td>\n",
       "      <td>39213900</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>governo indica josé mauro ferreira coelho pa...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>524 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Adj Close     Volume  Var%  Fechamento  \\\n",
       "0   2020-01-02      20.47   37774500  0.02           1   \n",
       "1   2020-01-03      20.30   71595600 -0.01           0   \n",
       "2   2020-01-06      20.54   81844000  0.01           1   \n",
       "3   2020-01-07      20.46   32822000 -0.00           0   \n",
       "5   2020-01-09      20.27   36102700 -0.00           0   \n",
       "..         ...        ...        ...   ...         ...   \n",
       "595 2022-05-27      30.60  118295600 -0.05           0   \n",
       "596 2022-05-30      29.99   98110100 -0.02           0   \n",
       "597 2022-05-31      30.06   83598000  0.00           1   \n",
       "598 2022-06-01      30.02   46353200 -0.00           0   \n",
       "601 2022-06-06      30.30   39213900  0.00           1   \n",
       "\n",
       "                                              Noticias  score  \n",
       "0     petrobras e vale retiram seus funcionários de...      0  \n",
       "1     petrobras reduz preços do diesel e da gasolin...      0  \n",
       "2      petrobras faz redução em produção de petróle...      3  \n",
       "3     o adeus da petrobras ao amazonas petrobras vê...     -6  \n",
       "5     refinarias da petrobras apresentam queda na c...     -3  \n",
       "..                                                 ...    ...  \n",
       "595   bolsonaro defende direito de mudar ceo da pet...     10  \n",
       "596    lira sugere que governo venda ações da petro...      1  \n",
       "597   bolsonaro diz que quer fatiar a petrobras bol...      6  \n",
       "598   petrobras diz que oferta de ações da braskem ...      3  \n",
       "601    governo indica josé mauro ferreira coelho pa...     -1  \n",
       "\n",
       "[524 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicionario_mes_2021 = {'Jan':'01', 'Fev':'02', 'Mar':'03','Abr':'04', 'Mai':'05', 'Jun':'06','Jul':'07', 'Ago':'08', 'Set':'09','Out':'10', 'Nov':'11', 'Dec':'12'}\n",
    "dicionario_mes_2020 = {'Jan':'01', 'Fev':'02', 'Mar':'03','Abr':'04', 'Mai':'05', 'Jun':'06','Jul':'07', 'Ago':'08', 'Set':'09','Out':'10', 'Nov':'11', 'Dez':'12'}\n",
    "dicionario_mes_2022 = {'Jan':'01', 'Fev':'02', 'Mar':'03','Abr':'04', 'Mai':'05', 'Jun':'06'}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in dicionario_mes_2020.keys():\n",
    "    arquivo = \"dataset-2020/\" + dicionario_mes_2020[i] + \"_GoogleNews_Petr_\" + i + \"-2020.csv\"\n",
    "    df_leitura = pd.read_csv(arquivo, sep='|')\n",
    "    df = df.append(df_leitura,ignore_index=True)\n",
    "\n",
    "for i in dicionario_mes_2021.keys():\n",
    "    arquivo = \"dataset-2021/\" + dicionario_mes_2021[i] + \"_GoogleNews_Petr_\" + i + \"_21.csv\"\n",
    "    df_leitura = pd.read_csv(arquivo, sep='|')\n",
    "    df = df.append(df_leitura,ignore_index=True)\n",
    "\n",
    "\n",
    "for i in dicionario_mes_2022.keys():\n",
    "    arquivo = \"dataset-2022/\" + dicionario_mes_2022[i] + \"_GoogleNews_Petr_\" + i + \"_22.csv\"\n",
    "    df_leitura = pd.read_csv(arquivo, sep='|')\n",
    "    df = df.append(df_leitura,ignore_index=True)\n",
    "\n",
    "#Transformando coluna data para datetime:\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "## Filtrando para pegar apenas os 3 primeiros meses de 2022\n",
    "#df = df[(df['date'] <= parser.parse('2022-03-31'))]\n",
    "\n",
    "\n",
    "## Lendo pregoes\n",
    "df_petro = pd.read_csv('dataset-2021/Hist_Preço_Petr_2021_.csv', sep='|')\n",
    "df_petro_2020 = pd.read_csv('dataset-2020/Hist_Preço_Petr_2020_.csv', sep='|')\n",
    "df_petro_2022 = pd.read_csv('dataset-2022/Hist_Preço_Petr_2022_.csv', sep='|')\n",
    "df_petro = df_petro_2020.append(df_petro,ignore_index=True)\n",
    "df_petro = df_petro.append(df_petro_2022,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#Transformando a coluna Date para datetime\n",
    "df_petro['Date'] = pd.to_datetime(df_petro['Date'])\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "## Gerando uma lista com todos os dias de 2021:\n",
    "start_date = '01/01/2020'\n",
    "end_date = '31/12/2022'\n",
    "\n",
    "#Transformando para o padrão inglês\n",
    "start_date = datetime.strptime(start_date, '%d/%m/%Y').strftime('%m-%d-%Y')\n",
    "end_date = datetime.strptime(end_date, '%d/%m/%Y').strftime('%m-%d-%Y')\n",
    "\n",
    "#Gerando a lista com todas as datas\n",
    "todas_datas = pd.date_range(start=start_date, end=end_date, freq = '1D')\n",
    "todas_datas = [i.strftime(\"%d/%m/%Y\") for i in todas_datas ]\n",
    "\n",
    "\n",
    "##Datas\n",
    "datas = df.date.value_counts()  \n",
    "data_df = datas.reset_index()\n",
    "data_df\n",
    "data_df['index'] = pd.to_datetime(data_df['index'])\n",
    "data_df.columns = ['Datas', 'Num_Noticias']\n",
    "data_df['Mes'] = data_df['Datas'].dt.month\n",
    "\n",
    "#Gerando lista com todas as datas com noticias\n",
    "datas_com_noticias = [i.strftime(\"%d/%m/%Y\") for i in data_df['Datas'] ]\n",
    "\n",
    "#Gerando lista com todas as datas sem noticias em 2021\n",
    "datas_sem_noticias = [i for i in todas_datas if i not in datas_com_noticias]\n",
    "\n",
    "datas_com_pregao = [i.strftime(\"%d/%m/%Y\") for i in df_petro['Date'] ]\n",
    "datas_sem_pregao = [i for i in todas_datas if i not in datas_com_pregao]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "df['title'] = df['title'].apply(lambda x: \"\" if \"petrobras\" not in x else x)\n",
    "df = df[(df['title'] != \"\")]\n",
    "\n",
    "df_petro['Fechamento'] = df_petro['Var%'].apply(lambda x: 0 if x<0 else 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_datas = []\n",
    "lista_news = []\n",
    "\n",
    "for i in df.date.unique():\n",
    "    news = \"\"\n",
    "    for row in df[(df['date']==i)].iterrows():\n",
    "        news = news + \" \" + row[1][0]\n",
    "    lista_news.append(news)\n",
    "    lista_datas.append(i)\n",
    "    \n",
    "    \n",
    "df_news_diaria = pd.DataFrame(list(zip(lista_datas,lista_news)),\n",
    "               columns =['Date', 'Noticias'])\n",
    "df_news_diaria.sort_values(by = 'Date', ascending = True, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iterar sobre as datas dos pregões (iniciando pelo segundo dia do pregão de 2021 df_petro.Date.iloc[1:])\n",
    "\n",
    "## Calcular delta (diferença entre dias entre dois registros seguidos de pregões):\n",
    "import datetime\n",
    "\n",
    "df_news_sem_pregao = pd.DataFrame()\n",
    "timedelta_1dia = datetime.timedelta(days=1)\n",
    "\n",
    "lista_datas = []\n",
    "lista_noticias_sem_pregao = []\n",
    "\n",
    "for i, data in enumerate(df_petro.Date.iloc[1:]):\n",
    "    data_anterior = df_petro['Date'].iloc[i]  \n",
    "    delta = data - data_anterior\n",
    "\n",
    "    \n",
    "    # Se houver mais de 1 dia sem pregão:    \n",
    "    if delta > timedelta_1dia:\n",
    "            \n",
    "            \n",
    "        # Filtra as noticias entre as datas sem pregão:\n",
    "        df_aux = df_news_diaria[ (df_news_diaria['Date']> data_anterior) & (df_news_diaria['Date']<= data)  ]\n",
    "        \n",
    "        ## Concatena as noticias das datas sem pregão\n",
    "        news = \"\"\n",
    "        for row in df_aux.iterrows():\n",
    "            news = news + \" \" + row[1][1]\n",
    "\n",
    "\n",
    "        ## Armazena as noticias e data do ultimo pregão valido em listas\n",
    "        lista_noticias_sem_pregao.append(news)\n",
    "        lista_datas.append(data)\n",
    "        \n",
    "        #Cria um dataframe auxiliar com a data do ultimo pregão e as noticias concatenadas dos dias sem pregões:\n",
    "        df_aux2 = pd.DataFrame(list(zip(lista_datas,lista_noticias_sem_pregao)),\n",
    "               columns =['Date', 'Noticias'])\n",
    "    \n",
    "        # Gera o dataframe com as noticias sem pregões + datas do ultimo pregão valido.\n",
    "        df_news_sem_pregao = df_news_sem_pregao.append(df_aux2, ignore_index = True)\n",
    "        \n",
    "        #Resetando as listas para geração de novo DF\n",
    "        lista_noticias_sem_pregao = []\n",
    "        lista_datas = []\n",
    "        \n",
    "\n",
    "df_news_diaria_atualizada = df_news_diaria.copy()\n",
    "\n",
    "# itera sobre os dias com pregão cujo noticias de dias anteriores foram concatenadas:\n",
    "for data in df_news_sem_pregao.Date.unique():\n",
    "    \n",
    "    #Filtra pelo dia com pregão que teve noticias concatenada\n",
    "    df_noticia_dias_sem_pregao = df_news_sem_pregao[(df_news_sem_pregao['Date']==data)]\n",
    "\n",
    "\n",
    "    #Checa se há registro referente a data no df de noticias\n",
    "    df_check_noticias = df_news_diaria_atualizada[(df_news_diaria_atualizada['Date']==data)]\n",
    "    \n",
    "    # Se não houver registros referente á data então o registro deverá ser criado no df de noticias:\n",
    "    # Se houver, então o registro será atualizado no df de noticias\n",
    "    \n",
    "    if len(df_check_noticias) > 0:\n",
    "        \n",
    "        #Substitui os registros\n",
    "        df_news_diaria_atualizada = df_news_diaria_atualizada.replace ((df_news_diaria_atualizada.loc[df_news_diaria_atualizada['Date'].isin(df_noticia_dias_sem_pregao['Date'])])['Noticias'].values, df_noticia_dias_sem_pregao['Noticias'].values)\n",
    "        \n",
    "    else:\n",
    "        #Insere o novo registro\n",
    "        df_news_diaria_atualizada = df_news_diaria_atualizada.append(df_noticia_dias_sem_pregao, ignore_index = True)\n",
    "        \n",
    "\n",
    "        \n",
    "df_final = pd.merge(left = df_petro, right = df_news_diaria_atualizada, how = 'left', on = 'Date')\n",
    "\n",
    "df_final = df_final.dropna()\n",
    "df_final = df_final[(df_final['Noticias'] != \"\")]\n",
    "\n",
    "\n",
    "\n",
    "#### Gerando features Sentilex\n",
    "\n",
    "\n",
    "sentilexpt = open('Versoes dicionarios sentilex/SentiLex-lem-PT01 editado v70_3_2022.txt','r',encoding='utf-8-sig')\n",
    "\n",
    "dic_palavra_polaridade = {}\n",
    "for i in sentilexpt.readlines():\n",
    "    pos_ponto = i.find('.')            # obtem a posiçãodo caracter ponto\n",
    "    palavra = (i[:pos_ponto])          # Pega a palavra\n",
    "    pol_pos = i.find('POL')            # obtem a posição do inicio da string POL\n",
    "    polaridade = (i[pol_pos+4:pol_pos+6]).replace(';','')         # obtem a polaridade da palavra\n",
    "    #polaridade = (i[pol_pos+4:pol_pos+7]).replace(';','')\n",
    "    dic_palavra_polaridade[palavra] = polaridade                  # atualiza o dicionario com a palavra a polaridade\n",
    "    \n",
    "\n",
    "def Score_sentimento(frase):\n",
    "    frase = frase.lower()                     # coloca toda a frase em minusculo\n",
    "    l_sentimento = []                         # cria uma lista vazia\n",
    "    for p in frase.split():\n",
    "        l_sentimento.append(int(dic_palavra_polaridade.get(p, 0)))      # para cada palavra obtem a polaridade\n",
    "        #l_sentimento.append(float(dic_palavra_polaridade.get(p, 0)))      # para cada palavra obtem a polaridade     \n",
    "    #print (l_sentimento)                                                # imprime a lista de polaridades\n",
    "    score = sum(l_sentimento)                                           # soma todos os valores da lista\n",
    "    #if score > 0:\n",
    "        #return 'Positivo, Score:{}'.format(score)                       # se maior que 0 retorna 'positivo'\n",
    "    #elif score == 0:\n",
    "        #return 'Neutro, Score:{}'.format(score)                         # se igual a 0 retorna 'neutro'\n",
    "    #else:\n",
    "        #return 'Negativo, Score:{}'.format(score)                       # se menor que 0 retorna 'negativo'\n",
    "        \n",
    "    return score\n",
    "\n",
    "df_final2 = df_final.copy()\n",
    "\n",
    "df_final2['score'] = df_final2['Noticias'].apply(lambda x: Score_sentimento(x))\n",
    "df_final2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traduzindo e gerando features Finbert e Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Traduzindo noticias\n",
    "from googletrans import Translator\n",
    "trans = Translator()\n",
    "def traduzir(frase):\n",
    "    frase = frase.lower()                     # coloca toda a frase em minusculo\n",
    "    frase = trans.translate(frase, dest = 'en').text\n",
    "    return frase\n",
    "\n",
    "df_final3 = df_final.copy()\n",
    "df_final3['Noticias'] = df_final3['Noticias'].apply(lambda x: traduzir(x))\n",
    "\n",
    "#### Gerando features Roberta\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def neu_rob(frase):\n",
    "    #trunca a frase para 514 caracteres (máximo suportado pelo modelo de Roberta)\n",
    "    frase = frase[:514]\n",
    "    \n",
    "    encoded_text = tokenizer(frase, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    neg_roberta = scores[0]\n",
    "   # neu_roberta = scores[1]\n",
    "   # pos_roberta = scores[2]\n",
    "    return neg_roberta\n",
    "\n",
    "df_final4 = df_final.copy()\n",
    "df_final4['Noticias'] = df_final3['Noticias']\n",
    "df_final4['neu_rob'] = df_final4['Noticias'].apply(lambda x: neu_rob(str(x)))\n",
    "\n",
    "\n",
    "#### Gerando features Finbert\n",
    "\n",
    "#pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "df_bert = df_final.copy()\n",
    "df_bert['Noticias'] = df_final3['Noticias']\n",
    "\n",
    "def sentimento_finbert_neg(string):\n",
    "    results = nlp([string])\n",
    "    dict_results = results[0]\n",
    "    sentimento = dict_results.get('label')\n",
    "    \n",
    "    if sentimento == \"Negative\":\n",
    "        score = -1*dict_results.get('score')\n",
    "    else:\n",
    "        score = 0\n",
    "        \n",
    "    return score\n",
    "\n",
    "df_bert['neg_finbert'] = df_bert['Noticias'].apply(lambda x: sentimento_finbert_neg(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando features de dias anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentilex + Vader\n",
    "df_final8 = df_final3.merge(df_final2, how = 'left', on = ['Date', 'Adj Close', 'Volume', 'Var%', 'Fechamento'])\n",
    "# + Roberta\n",
    "df_final8 = df_final8.merge(df_final4, how = 'left', on = ['Date', 'Adj Close', 'Volume', 'Var%', 'Fechamento'])\n",
    "# +Finbert\n",
    "df_final8 = df_final8.merge(df_bert, how = 'left', on = ['Date', 'Adj Close', 'Volume', 'Var%', 'Fechamento'])\n",
    "\n",
    "\n",
    "df_final99 = df_final8.copy()\n",
    "\n",
    "features = ['score', 'neu_rob','neg_finbert']\n",
    "featuresd1 = [i + \"d1\" for i in features]\n",
    "featuresd2 = [i + \"d2\" for i in features]\n",
    "featuresd3 = [i + \"d3\" for i in features]\n",
    "featuresd4 = [i + \"d4\" for i in features]\n",
    "\n",
    "#Criando as colunas de features para d-1, d-2, d-3, d-4 e inicializando com valores zeros:\n",
    "for i in features:\n",
    "    df_final99[i+\"d1\"] = 0\n",
    "    df_final99[i+\"d2\"] = 0\n",
    "    df_final99[i+\"d3\"] = 0\n",
    "    df_final99[i+\"d4\"] = 0\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "#atualiza as features de d-1\n",
    "for a,b in itertools.zip_longest(features,featuresd1):\n",
    "    df_final99[b] = df_final99.shift(periods=1)[a]\n",
    "    \n",
    "#atualiza as features de d-2\n",
    "for a,b in itertools.zip_longest(features,featuresd2):\n",
    "    df_final99[b] = df_final99.shift(periods=2)[a]\n",
    "    \n",
    "#atualiza as features de d-3\n",
    "for a,b in itertools.zip_longest(features,featuresd3):\n",
    "    df_final99[b] = df_final99.shift(periods=3)[a]\n",
    "    \n",
    "#atualiza as features de d-4\n",
    "for a,b in itertools.zip_longest(features,featuresd4):\n",
    "    df_final99[b] = df_final99.shift(periods=4)[a]\n",
    "\n",
    "\n",
    "df_final99 = df_final99.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Retreinando todos modelos de ML e Ensemble dos experimentos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', metric='manhattan', n_neighbors=3,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treinar_ate_data_str = '2022-01-15'\n",
    "\n",
    "X = df_final99[(df_final99['Date'] > parser.parse(treinar_ate_data_str))][['score','neu_robd4','neg_finbertd2','scored3']]\n",
    "y = df_final99[(df_final99['Date'] > parser.parse(treinar_ate_data_str))][['Fechamento']]\n",
    "\n",
    "#holdout\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=7, stratify=y)\n",
    "\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "#Treinamento dos modelos do ensemble:\n",
    "linearsvc =  LinearSVC(penalty='l2', loss='hinge',dual=True, C=1.4)\n",
    "linearsvc.fit(X_train,y_train)\n",
    "\n",
    "ridgecv = RidgeClassifierCV(alphas = [0.001, 0.01, 0.1, 1], fit_intercept=True, normalize=True, cv=12)\n",
    "ridgecv.fit(X_train,y_train)\n",
    "\n",
    "logreg = LogisticRegression(penalty= 'l1', C=28, fit_intercept=True, solver = 'liblinear')\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "ridge = RidgeClassifier(alpha=1.7,fit_intercept=True,normalize=False, solver = 'lsqr' )\n",
    "ridge.fit(X_train,y_train)\n",
    "\n",
    "## Treinamento do Ensemble\n",
    "clfs = [logreg,ridge,ridgecv,linearsvc]\n",
    "ensemblevote = EnsembleVoteClassifier(clfs, weights = [2,1,2,1]) \n",
    "ensemblevote.fit(X_train,y_train)\n",
    "\n",
    "nearestcentroid = NearestCentroid(metric = 'euclidean', shrink_threshold = 0.18)\n",
    "nearestcentroid.fit(X_train,y_train)\n",
    "\n",
    "svc = SVC(C=1.46, kernel = 'linear', gamma = 'auto')\n",
    "svc.fit(X_train,y_train)\n",
    "    \n",
    "lineardisc = LinearDiscriminantAnalysis(solver = 'eigen', n_components = 1)\n",
    "lineardisc.fit(X_train,y_train)\n",
    "\n",
    "bernoullinb = BernoulliNB(alpha=100,binarize=0,fit_prior=True)\n",
    "bernoullinb.fit(X_train,y_train)\n",
    "\n",
    "gaussiannb = GaussianNB(var_smoothing = 0.0001)\n",
    "gaussiannb.fit(X_train,y_train)\n",
    "\n",
    "lgbm = LGBMClassifier(max_depth=180,learning_rate=0.01,n_estimators=250, num_leaves=5)\n",
    "lgbm.fit(X_train,y_train)\n",
    "\n",
    "extratree = ExtraTreeClassifier(criterion = 'entropy', splitter = 'best', max_depth = 25, min_samples_split=20, min_samples_leaf=2)\n",
    "extratree.fit(X_train,y_train)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=10, min_samples_split=15,min_samples_leaf=1,criterion='entropy')\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "xgb = XGBClassifier(max_depth=3,learning_rate=0.001,n_estimators=86)\n",
    "xgb.fit(X_train,y_train)\n",
    "\n",
    "quadratic = QuadraticDiscriminantAnalysis(reg_param=0,store_covariance=True, tol=0.0000000001)\n",
    "quadratic.fit(X_train,y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, weights = 'distance', algorithm = 'brute', metric = 'manhattan')\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetição 1\n",
      "Repetição 2\n",
      "Repetição 3\n",
      "Repetição 4\n",
      "Repetição 5\n",
      "Repetição 6\n",
      "Repetição 7\n",
      "Repetição 8\n",
      "Repetição 9\n",
      "Repetição 10\n",
      "Repetição 11\n",
      "Repetição 12\n",
      "Repetição 13\n",
      "Repetição 14\n",
      "Repetição 15\n",
      "Repetição 16\n",
      "Repetição 17\n",
      "Repetição 18\n",
      "Repetição 19\n",
      "Repetição 20\n",
      "Repetição 21\n",
      "Repetição 22\n",
      "Repetição 23\n",
      "Repetição 24\n",
      "Repetição 25\n",
      "Repetição 26\n",
      "Repetição 27\n",
      "Repetição 28\n",
      "Repetição 29\n",
      "Repetição 30\n",
      "Results nearestcentroid: 0.4913 (0.0799) balanced_accuracy\n",
      "Results svc: 0.4862 (0.0791) balanced_accuracy\n",
      "Results lineardisc: 0.4953 (0.0632) balanced_accuracy\n",
      "Results ridge: 0.4887 (0.0721) balanced_accuracy\n",
      "Results linearsvc: 0.4859 (0.0751) balanced_accuracy\n",
      "Results ridgecv: 0.4905 (0.0647) balanced_accuracy\n",
      "Results logreg: 0.4866 (0.0733) balanced_accuracy\n",
      "Results bernoullinb: 0.4861 (0.0463) balanced_accuracy\n",
      "Results gaussiannb: 0.4481 (0.0806) balanced_accuracy\n",
      "Results lgbm: 0.4667 (0.0841) balanced_accuracy\n",
      "Results extratee: 0.4821 (0.0943) balanced_accuracy\n",
      "Results rf: 0.4755 (0.0670) balanced_accuracy\n",
      "Results xgb: 0.5552 (0.0888) balanced_accuracy\n",
      "Results quadratic: 0.4863 (0.0752) balanced_accuracy\n",
      "Results knn: 0.5887 (0.0800) balanced_accuracy\n",
      "Results ensemble: 0.4884 (0.0722) balanced_accuracy\n"
     ]
    }
   ],
   "source": [
    "resultados_nearestcentroid = []\n",
    "resultados_svc = []\n",
    "resultados_lineardisc = []\n",
    "resultados_ridge = []\n",
    "resultados_linearsvc = []\n",
    "resultados_ridgecv = []\n",
    "resultados_logreg = []\n",
    "resultados_bernoullinb = []\n",
    "resultados_gaussiannb = []\n",
    "resultados_lgbm = []\n",
    "resultados_extratree = []\n",
    "resultados_rf = []\n",
    "resultados_xgb = []\n",
    "resultados_quadratic = []\n",
    "resultados_knn = []\n",
    "resultados_ensemble = []\n",
    "\n",
    "for i in range(30):\n",
    "    print(\"Repetição %s\" % (i+1))\n",
    "    \n",
    "    #Holdout\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i, stratify=y)\n",
    "\n",
    "    \n",
    "    # 10 Kfolds\n",
    "    kfold = KFold(n_splits = 10, shuffle = True, random_state = i)\n",
    "    \n",
    "    \n",
    "    # Aplicação dos kfolds para avaliação estatística\n",
    "    scores1 = cross_val_score(nearestcentroid, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores2 = cross_val_score(svc, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores3 = cross_val_score(lineardisc, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores4 = cross_val_score(ridge, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores5 = cross_val_score(linearsvc, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores6 = cross_val_score(ridgecv, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores7 = cross_val_score(logreg, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores8 = cross_val_score(bernoullinb, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores9 = cross_val_score(gaussiannb, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores10 = cross_val_score(lgbm, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores11 = cross_val_score(extratree, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores12 = cross_val_score(rf, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores13 = cross_val_score(xgb, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores14 = cross_val_score(quadratic, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores15 = cross_val_score(knn, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    scores16 = cross_val_score(ensemblevote, X_train, y_train, cv = kfold, scoring = \"balanced_accuracy\")\n",
    "    \n",
    "    #Armazenando a média da acurácia balanceada para cada algoritmo - TEOREMA CENTRAL DO LIMITE - Importante para avaliação estatistica\n",
    "    resultados_nearestcentroid.append(scores1.mean())   \n",
    "    resultados_svc.append(scores2.mean()) \n",
    "    resultados_lineardisc.append(scores3.mean()) \n",
    "    resultados_ridge.append(scores4.mean()) \n",
    "    resultados_linearsvc.append(scores5.mean()) \n",
    "    resultados_ridgecv.append(scores6.mean()) \n",
    "    resultados_logreg.append(scores7.mean()) \n",
    "    resultados_bernoullinb.append(scores8.mean()) \n",
    "    resultados_gaussiannb.append(scores9.mean()) \n",
    "    resultados_lgbm.append(scores10.mean()) \n",
    "    resultados_extratree.append(scores11.mean()) \n",
    "    resultados_rf.append(scores12.mean()) \n",
    "    resultados_xgb.append(scores13.mean()) \n",
    "    resultados_quadratic.append(scores14.mean()) \n",
    "    resultados_knn.append(scores15.mean()) \n",
    "    resultados_ensemble.append(scores16.mean()) \n",
    "    \n",
    "## transformando os resultados em array numpy\n",
    "\n",
    "scores1 = np.array(resultados_nearestcentroid)\n",
    "scores2 = np.array(resultados_svc)\n",
    "scores3 = np.array(resultados_lineardisc)\n",
    "scores4 = np.array(resultados_ridge)\n",
    "scores5 = np.array(resultados_linearsvc)\n",
    "scores6 = np.array(resultados_ridgecv)\n",
    "scores7 = np.array(resultados_logreg)\n",
    "scores8 = np.array(resultados_bernoullinb)\n",
    "scores9 = np.array(resultados_gaussiannb)\n",
    "scores10 = np.array(resultados_lgbm)\n",
    "scores11 = np.array(resultados_extratree)\n",
    "scores12 = np.array(resultados_rf)\n",
    "scores13 = np.array(resultados_xgb)\n",
    "scores14 = np.array(resultados_quadratic)\n",
    "scores15 = np.array(resultados_knn)\n",
    "scores16 = np.array(resultados_ensemble)\n",
    "\n",
    "\n",
    "print(\"Results nearestcentroid: %.4f (%.4f) balanced_accuracy\" % (scores1.mean(), scores1.std()))\n",
    "print(\"Results svc: %.4f (%.4f) balanced_accuracy\" % (scores2.mean(), scores2.std()))\n",
    "print(\"Results lineardisc: %.4f (%.4f) balanced_accuracy\" % (scores3.mean(), scores3.std()))\n",
    "print(\"Results ridge: %.4f (%.4f) balanced_accuracy\" % (scores4.mean(), scores4.std()))\n",
    "print(\"Results linearsvc: %.4f (%.4f) balanced_accuracy\" % (scores5.mean(), scores5.std()))\n",
    "print(\"Results ridgecv: %.4f (%.4f) balanced_accuracy\" % (scores6.mean(), scores6.std()))\n",
    "print(\"Results logreg: %.4f (%.4f) balanced_accuracy\" % (scores7.mean(), scores7.std()))\n",
    "print(\"Results bernoullinb: %.4f (%.4f) balanced_accuracy\" % (scores8.mean(), scores8.std()))\n",
    "print(\"Results gaussiannb: %.4f (%.4f) balanced_accuracy\" % (scores9.mean(), scores9.std()))\n",
    "print(\"Results lgbm: %.4f (%.4f) balanced_accuracy\" % (scores10.mean(), scores10.std()))\n",
    "print(\"Results extratee: %.4f (%.4f) balanced_accuracy\" % (scores11.mean(), scores11.std()))\n",
    "print(\"Results rf: %.4f (%.4f) balanced_accuracy\" % (scores12.mean(), scores12.std()))\n",
    "print(\"Results xgb: %.4f (%.4f) balanced_accuracy\" % (scores13.mean(), scores13.std()))\n",
    "print(\"Results quadratic: %.4f (%.4f) balanced_accuracy\" % (scores14.mean(), scores14.std()))\n",
    "print(\"Results knn: %.4f (%.4f) balanced_accuracy\" % (scores15.mean(), scores15.std()))\n",
    "print(\"Results ensemble: %.4f (%.4f) balanced_accuracy\" % (scores16.mean(), scores16.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando matriz de confusao e outras métricas nos dados de 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report Dados:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87        38\n",
      "           1       0.88      0.88      0.88        41\n",
      "\n",
      "    accuracy                           0.87        79\n",
      "   macro avg       0.87      0.87      0.87        79\n",
      "weighted avg       0.87      0.87      0.87        79\n",
      "\n",
      "\n",
      "Confusion Matrix Dados de Producao:\n",
      " [[33  5]\n",
      " [ 5 36]]\n",
      "\n",
      "Recall_0: 0.868421052631579\n",
      "Recall_1: 0.8780487804878049\n",
      "Acuracia: 0.8734177215189873\n",
      "Acuracia Balanceada: 0.873234916559692\n",
      "F1_Score_0: 0.868421052631579\n",
      "F1_Score_1: 0.8780487804878049\n",
      "AUC: 0.8732349165596919\n"
     ]
    }
   ],
   "source": [
    "#Predizendo y dados após a data de treinamento\n",
    "\n",
    "X = df_final99[(df_final99['Date'] > parser.parse(treinar_ate_data_str))][['score','neu_robd4','neg_finbertd2','scored3']]\n",
    "y = df_final99[(df_final99['Date'] > parser.parse(treinar_ate_data_str))][['Fechamento']]\n",
    "\n",
    "y_pred = knn.predict(X)\n",
    "\n",
    "#Relatórios e matriz de confusao\n",
    "print(\"Classification Report Dados:\\n\",classification_report(y,y_pred))\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix Dados de Producao:\\n\",confusion_matrix(y,y_pred))\n",
    "print(\"\")\n",
    "\n",
    "#Calculando o recall\n",
    "recall_0 = recall_score(y, y_pred, pos_label=0)\n",
    "recall_1 = recall_score(y, y_pred, pos_label=1)\n",
    "\n",
    "#Calculando acurácia \n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "#Calculando acurácia balanceada\n",
    "accuracy_balanced = balanced_accuracy_score(y, y_pred)\n",
    "\n",
    "#calculando f1_score\n",
    "f1_0 = f1_score(y, y_pred,pos_label=0)\n",
    "f1_1 = f1_score(y, y_pred,pos_label=1)\n",
    "\n",
    "#Calculando AUC\n",
    "auc = roc_auc_score(y, y_pred)\n",
    "\n",
    "print(\"Recall_0: %s\" % (recall_0))\n",
    "print(\"Recall_1: %s\" % (recall_1))\n",
    "print(\"Acuracia: %s\" % (accuracy))\n",
    "print(\"Acuracia Balanceada: %s\" % (accuracy_balanced))\n",
    "print(\"F1_Score_0: %s\" % (f1_0))\n",
    "print(\"F1_Score_1: %s\" % (f1_1))\n",
    "print(\"AUC: %s\" % (auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Tunagem modelos - Treinando os modelos com dados de 2020,2021 e avaliando com dados de 2022:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinar_ate_data_str='2021-12-31'\n",
    "\n",
    "# dados de 2020 e 2021\n",
    "X = df_final99[(df_final99['Date'] <= parser.parse(treinar_ate_data_str))][['score','neu_robd4','neg_finbertd2','scored3']]\n",
    "y = df_final99[(df_final99['Date'] <= parser.parse(treinar_ate_data_str))][['Fechamento']]\n",
    "\n",
    "# dados de 2022\n",
    "X_test = df_final99[(df_final99['Date'] <= parser.parse(treinar_ate_data_str))][['score','neu_robd4','neg_finbertd2','scored3']]\n",
    "y_test = df_final99[(df_final99['Date'] <= parser.parse(treinar_ate_data_str))][['Fechamento']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GLQVL39etxA7",
    "w1XbpuI2txBD",
    "uVhYASlKtxBE",
    "kA8xUq80txBG",
    "WvunB2T2txBH",
    "yaZKt1aQtxBH",
    "o0FDRw9otxBI",
    "4Ps-TQJZtxBK",
    "Hvi7sE4mtxBK",
    "EnI0cgNDtxBL",
    "O2JRt_catxBL",
    "0t2vvLQstxBM",
    "WFxO_oNltxBN",
    "MR3swdq4txBO",
    "C6yWLR4StxBO",
    "BQF6Nd4NtxBW",
    "oqvY9hq6txBW",
    "O_VKfbbptxBX",
    "Q2aQq_0ZtxBY",
    "mmSDP734txBZ",
    "EVKPJo8KtxBZ",
    "gHYaLzzAtxBa",
    "u4_fMM9otxBa",
    "UMDvNtcitxBb",
    "TAmsdvUwtxBd",
    "mR1zw3eJtxBe",
    "F4t54GmWtxBf",
    "I-i7Chs1txBg",
    "DVLZJzx-txBh",
    "jSZ3ULpJtxBi",
    "c329l7BptxBj",
    "csFs7BZWtxBl",
    "NpEryjovtxBl",
    "UXo3Ahr0txBp",
    "aX2TRyThtxBr",
    "aIHcMoYXtxBt",
    "0bP1g5m0txBu",
    "7qGStQbAtxBw",
    "kI0uCb6ttxBz",
    "YvVsjbrptxB4",
    "AUrsC36KtxB-",
    "Gji2KE9ntxCB",
    "VIn0WCHTtxCC",
    "fZ134YHCtxCD",
    "d9p5fXTGtxCE",
    "MIeCms0NtxCF"
   ],
   "name": " Experimentos_+_EDA_(5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
