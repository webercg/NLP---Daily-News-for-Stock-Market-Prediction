{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/webercg/NLP---Daily-News-for-Stock-Market-Prediction/blob/main/Experimentos_%2B_EDA_(5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aMeKxD1txAj"
   },
   "source": [
    "# 1.0 Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PN29buKktxAm"
   },
   "outputs": [],
   "source": [
    "#Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Visualização de dados\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Manipulação datas\n",
    "from datetime import datetime\n",
    "\n",
    "# Prototipação\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "\n",
    "#Pipeline e pré-process\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import NuSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "#Model Tunning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Model evaluation\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Gerando dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando dataframe e gerando features Sentilex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Var%</th>\n",
       "      <th>Fechamento</th>\n",
       "      <th>Noticias</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>20.466219</td>\n",
       "      <td>37774500</td>\n",
       "      <td>0.017230</td>\n",
       "      <td>1</td>\n",
       "      <td>petrobras e vale retiram seus funcionários de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>20.299557</td>\n",
       "      <td>71595600</td>\n",
       "      <td>-0.008143</td>\n",
       "      <td>0</td>\n",
       "      <td>petrobras reduz preços do diesel e da gasolin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>20.539553</td>\n",
       "      <td>81844000</td>\n",
       "      <td>0.011823</td>\n",
       "      <td>1</td>\n",
       "      <td>petrobras faz redução em produção de petróle...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>20.459551</td>\n",
       "      <td>32822000</td>\n",
       "      <td>-0.003895</td>\n",
       "      <td>0</td>\n",
       "      <td>o adeus da petrobras ao amazonas petrobras vê...</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>20.266226</td>\n",
       "      <td>36102700</td>\n",
       "      <td>-0.003279</td>\n",
       "      <td>0</td>\n",
       "      <td>refinarias da petrobras apresentam queda na c...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2022-06-24</td>\n",
       "      <td>26.290001</td>\n",
       "      <td>53413400</td>\n",
       "      <td>-0.007550</td>\n",
       "      <td>0</td>\n",
       "      <td>comitê de elegibilidade da petrobras dá aval ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>2022-06-27</td>\n",
       "      <td>27.980000</td>\n",
       "      <td>90417700</td>\n",
       "      <td>0.064283</td>\n",
       "      <td>1</td>\n",
       "      <td>caio andrade nega recomendação do governo pa...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>2022-06-28</td>\n",
       "      <td>28.330000</td>\n",
       "      <td>51388000</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>1</td>\n",
       "      <td>petrobras: novo presidente não conseguirá mud...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>2022-06-29</td>\n",
       "      <td>28.080000</td>\n",
       "      <td>52048800</td>\n",
       "      <td>-0.008825</td>\n",
       "      <td>0</td>\n",
       "      <td>paes de andrade falta à primeira reunião do c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>27.930000</td>\n",
       "      <td>49910100</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>0</td>\n",
       "      <td>caio paes de andrade: como um dos investidore...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Adj Close    Volume      Var%  Fechamento  \\\n",
       "0   2020-01-02  20.466219  37774500  0.017230           1   \n",
       "1   2020-01-03  20.299557  71595600 -0.008143           0   \n",
       "2   2020-01-06  20.539553  81844000  0.011823           1   \n",
       "3   2020-01-07  20.459551  32822000 -0.003895           0   \n",
       "5   2020-01-09  20.266226  36102700 -0.003279           0   \n",
       "..         ...        ...       ...       ...         ...   \n",
       "614 2022-06-24  26.290001  53413400 -0.007550           0   \n",
       "615 2022-06-27  27.980000  90417700  0.064283           1   \n",
       "616 2022-06-28  28.330000  51388000  0.012509           1   \n",
       "617 2022-06-29  28.080000  52048800 -0.008825           0   \n",
       "618 2022-06-30  27.930000  49910100 -0.005342           0   \n",
       "\n",
       "                                              Noticias  score  \n",
       "0     petrobras e vale retiram seus funcionários de...      0  \n",
       "1     petrobras reduz preços do diesel e da gasolin...      0  \n",
       "2      petrobras faz redução em produção de petróle...      3  \n",
       "3     o adeus da petrobras ao amazonas petrobras vê...     -6  \n",
       "5     refinarias da petrobras apresentam queda na c...     -3  \n",
       "..                                                 ...    ...  \n",
       "614   comitê de elegibilidade da petrobras dá aval ...      3  \n",
       "615    caio andrade nega recomendação do governo pa...      2  \n",
       "616   petrobras: novo presidente não conseguirá mud...      9  \n",
       "617   paes de andrade falta à primeira reunião do c...      0  \n",
       "618   caio paes de andrade: como um dos investidore...      1  \n",
       "\n",
       "[538 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicionario_mes_2021 = {'Jan':'01', 'Fev':'02', 'Mar':'03','Abr':'04', 'Mai':'05', 'Jun':'06','Jul':'07', 'Ago':'08', 'Set':'09','Out':'10', 'Nov':'11', 'Dec':'12'}\n",
    "dicionario_mes_2020 = {'Jan':'01', 'Fev':'02', 'Mar':'03','Abr':'04', 'Mai':'05', 'Jun':'06','Jul':'07', 'Ago':'08', 'Set':'09','Out':'10', 'Nov':'11', 'Dez':'12'}\n",
    "dicionario_mes_2022 = {'Jan':'01', 'Fev':'02', 'Mar':'03','Abr':'04', 'Mai':'05', 'Jun':'06'}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in dicionario_mes_2020.keys():\n",
    "    arquivo = \"dataset-2020/\" + dicionario_mes_2020[i] + \"_GoogleNews_Petr_\" + i + \"-2020.csv\"\n",
    "    df_leitura = pd.read_csv(arquivo, sep='|')\n",
    "    df = df.append(df_leitura,ignore_index=True)\n",
    "\n",
    "for i in dicionario_mes_2021.keys():\n",
    "    arquivo = \"dataset-2021/\" + dicionario_mes_2021[i] + \"_GoogleNews_Petr_\" + i + \"_21.csv\"\n",
    "    df_leitura = pd.read_csv(arquivo, sep='|')\n",
    "    df = df.append(df_leitura,ignore_index=True)\n",
    "\n",
    "\n",
    "for i in dicionario_mes_2022.keys():\n",
    "    arquivo = \"dataset-2022/\" + dicionario_mes_2022[i] + \"_GoogleNews_Petr_\" + i + \"_22.csv\"\n",
    "    df_leitura = pd.read_csv(arquivo, sep='|')\n",
    "    df = df.append(df_leitura,ignore_index=True)\n",
    "\n",
    "#Transformando coluna data para datetime:\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "## Filtrando para pegar apenas os 3 primeiros meses de 2022\n",
    "#df = df[(df['date'] <= parser.parse('2022-03-31'))]\n",
    "\n",
    "\n",
    "## Lendo pregoes\n",
    "df_petro = pd.read_csv('dataset-2021/Hist_Preço_Petr_2021_.csv', sep='|')\n",
    "df_petro_2020 = pd.read_csv('dataset-2020/Hist_Preço_Petr_2020_.csv', sep='|')\n",
    "df_petro_2022 = pd.read_csv('dataset-2022/Hist_Preço_Petr_2022_.csv', sep='|')\n",
    "df_petro = df_petro_2020.append(df_petro,ignore_index=True)\n",
    "df_petro = df_petro.append(df_petro_2022,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "#Transformando a coluna Date para datetime\n",
    "df_petro['Date'] = pd.to_datetime(df_petro['Date'])\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "## Gerando uma lista com todos os dias:\n",
    "start_date = '01/01/2020'\n",
    "end_date = '31/12/2022'\n",
    "\n",
    "#Transformando para o padrão inglês\n",
    "start_date = datetime.strptime(start_date, '%d/%m/%Y').strftime('%m-%d-%Y')\n",
    "end_date = datetime.strptime(end_date, '%d/%m/%Y').strftime('%m-%d-%Y')\n",
    "\n",
    "#Gerando a lista com todas as datas\n",
    "todas_datas = pd.date_range(start=start_date, end=end_date, freq = '1D')\n",
    "todas_datas = [i.strftime(\"%d/%m/%Y\") for i in todas_datas ]\n",
    "\n",
    "\n",
    "##Datas\n",
    "datas = df.date.value_counts()  \n",
    "data_df = datas.reset_index()\n",
    "data_df\n",
    "data_df['index'] = pd.to_datetime(data_df['index'])\n",
    "data_df.columns = ['Datas', 'Num_Noticias']\n",
    "data_df['Mes'] = data_df['Datas'].dt.month\n",
    "\n",
    "#Gerando lista com todas as datas com noticias\n",
    "datas_com_noticias = [i.strftime(\"%d/%m/%Y\") for i in data_df['Datas'] ]\n",
    "\n",
    "#Gerando lista com todas as datas sem noticias em 2021\n",
    "datas_sem_noticias = [i for i in todas_datas if i not in datas_com_noticias]\n",
    "\n",
    "datas_com_pregao = [i.strftime(\"%d/%m/%Y\") for i in df_petro['Date'] ]\n",
    "datas_sem_pregao = [i for i in todas_datas if i not in datas_com_pregao]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['title'] = df['title'].apply(lambda x: x.lower())\n",
    "df['title'] = df['title'].apply(lambda x: \"\" if \"petrobras\" not in x else x)\n",
    "df = df[(df['title'] != \"\")]\n",
    "\n",
    "df_petro['Fechamento'] = df_petro['Var%'].apply(lambda x: 0 if x<0 else 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_datas = []\n",
    "lista_news = []\n",
    "\n",
    "for i in df.date.unique():\n",
    "    news = \"\"\n",
    "    for row in df[(df['date']==i)].iterrows():\n",
    "        news = news + \" \" + row[1][0]\n",
    "    lista_news.append(news)\n",
    "    lista_datas.append(i)\n",
    "    \n",
    "    \n",
    "df_news_diaria = pd.DataFrame(list(zip(lista_datas,lista_news)),\n",
    "               columns =['Date', 'Noticias'])\n",
    "df_news_diaria.sort_values(by = 'Date', ascending = True, inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Iterar sobre as datas dos pregões (iniciando pelo segundo dia do pregão de 2021 df_petro.Date.iloc[1:])\n",
    "\n",
    "## Calcular delta (diferença entre dias entre dois registros seguidos de pregões):\n",
    "import datetime\n",
    "\n",
    "df_news_sem_pregao = pd.DataFrame()\n",
    "timedelta_1dia = datetime.timedelta(days=1)\n",
    "\n",
    "lista_datas = []\n",
    "lista_noticias_sem_pregao = []\n",
    "\n",
    "for i, data in enumerate(df_petro.Date.iloc[1:]):\n",
    "    data_anterior = df_petro['Date'].iloc[i]  \n",
    "    delta = data - data_anterior\n",
    "\n",
    "    \n",
    "    # Se houver mais de 1 dia sem pregão:    \n",
    "    if delta > timedelta_1dia:\n",
    "            \n",
    "            \n",
    "        # Filtra as noticias entre as datas sem pregão:\n",
    "        df_aux = df_news_diaria[ (df_news_diaria['Date']> data_anterior) & (df_news_diaria['Date']<= data)  ]\n",
    "        \n",
    "        ## Concatena as noticias das datas sem pregão\n",
    "        news = \"\"\n",
    "        for row in df_aux.iterrows():\n",
    "            news = news + \" \" + row[1][1]\n",
    "\n",
    "\n",
    "        ## Armazena as noticias e data do ultimo pregão valido em listas\n",
    "        lista_noticias_sem_pregao.append(news)\n",
    "        lista_datas.append(data)\n",
    "        \n",
    "        #Cria um dataframe auxiliar com a data do ultimo pregão e as noticias concatenadas dos dias sem pregões:\n",
    "        df_aux2 = pd.DataFrame(list(zip(lista_datas,lista_noticias_sem_pregao)),\n",
    "               columns =['Date', 'Noticias'])\n",
    "    \n",
    "        # Gera o dataframe com as noticias sem pregões + datas do ultimo pregão valido.\n",
    "        df_news_sem_pregao = df_news_sem_pregao.append(df_aux2, ignore_index = True)\n",
    "        \n",
    "        #Resetando as listas para geração de novo DF\n",
    "        lista_noticias_sem_pregao = []\n",
    "        lista_datas = []\n",
    "        \n",
    "\n",
    "df_news_diaria_atualizada = df_news_diaria.copy()\n",
    "\n",
    "# itera sobre os dias com pregão cujo noticias de dias anteriores foram concatenadas:\n",
    "for data in df_news_sem_pregao.Date.unique():\n",
    "    \n",
    "    #Filtra pelo dia com pregão que teve noticias concatenada\n",
    "    df_noticia_dias_sem_pregao = df_news_sem_pregao[(df_news_sem_pregao['Date']==data)]\n",
    "\n",
    "\n",
    "    #Checa se há registro referente a data no df de noticias\n",
    "    df_check_noticias = df_news_diaria_atualizada[(df_news_diaria_atualizada['Date']==data)]\n",
    "    \n",
    "    # Se não houver registros referente á data então o registro deverá ser criado no df de noticias:\n",
    "    # Se houver, então o registro será atualizado no df de noticias\n",
    "    \n",
    "    if len(df_check_noticias) > 0:\n",
    "        \n",
    "        #Substitui os registros\n",
    "        df_news_diaria_atualizada = df_news_diaria_atualizada.replace ((df_news_diaria_atualizada.loc[df_news_diaria_atualizada['Date'].isin(df_noticia_dias_sem_pregao['Date'])])['Noticias'].values, df_noticia_dias_sem_pregao['Noticias'].values)\n",
    "        \n",
    "    else:\n",
    "        #Insere o novo registro\n",
    "        df_news_diaria_atualizada = df_news_diaria_atualizada.append(df_noticia_dias_sem_pregao, ignore_index = True)\n",
    "        \n",
    "\n",
    "        \n",
    "df_final = pd.merge(left = df_petro, right = df_news_diaria_atualizada, how = 'left', on = 'Date')\n",
    "\n",
    "df_final = df_final.dropna()\n",
    "df_final = df_final[(df_final['Noticias'] != \"\")]\n",
    "\n",
    "\n",
    "\n",
    "#### Gerando features Sentilex\n",
    "\n",
    "\n",
    "sentilexpt = open('Versoes dicionarios sentilex/SentiLex-lem-PT01 editado v70_3.txt','r',encoding='utf-8-sig')\n",
    "\n",
    "dic_palavra_polaridade = {}\n",
    "for i in sentilexpt.readlines():\n",
    "    pos_ponto = i.find('.')            # obtem a posiçãodo caracter ponto\n",
    "    palavra = (i[:pos_ponto])          # Pega a palavra\n",
    "    pol_pos = i.find('POL')            # obtem a posição do inicio da string POL\n",
    "    polaridade = (i[pol_pos+4:pol_pos+6]).replace(';','')         # obtem a polaridade da palavra\n",
    "    #polaridade = (i[pol_pos+4:pol_pos+7]).replace(';','')\n",
    "    dic_palavra_polaridade[palavra] = polaridade                  # atualiza o dicionario com a palavra a polaridade\n",
    "    \n",
    "\n",
    "def Score_sentimento(frase):\n",
    "    frase = frase.lower()                     # coloca toda a frase em minusculo\n",
    "    l_sentimento = []                         # cria uma lista vazia\n",
    "    for p in frase.split():\n",
    "        l_sentimento.append(int(dic_palavra_polaridade.get(p, 0)))      # para cada palavra obtem a polaridade\n",
    "        #l_sentimento.append(float(dic_palavra_polaridade.get(p, 0)))      # para cada palavra obtem a polaridade     \n",
    "    #print (l_sentimento)                                                # imprime a lista de polaridades\n",
    "    score = sum(l_sentimento)                                           # soma todos os valores da lista\n",
    "    #if score > 0:\n",
    "        #return 'Positivo, Score:{}'.format(score)                       # se maior que 0 retorna 'positivo'\n",
    "    #elif score == 0:\n",
    "        #return 'Neutro, Score:{}'.format(score)                         # se igual a 0 retorna 'neutro'\n",
    "    #else:\n",
    "        #return 'Negativo, Score:{}'.format(score)                       # se menor que 0 retorna 'negativo'\n",
    "        \n",
    "    return score\n",
    "\n",
    "df_final2 = df_final.copy()\n",
    "\n",
    "df_final2['score'] = df_final2['Noticias'].apply(lambda x: Score_sentimento(x))\n",
    "df_final2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traduzindo e gerando features Finbert e Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Traduzindo noticias\n",
    "from googletrans import Translator\n",
    "trans = Translator()\n",
    "def traduzir(frase):\n",
    "    frase = frase.lower()                     # coloca toda a frase em minusculo\n",
    "    frase = trans.translate(frase, dest = 'en').text\n",
    "    return frase\n",
    "\n",
    "df_final3 = df_final.copy()\n",
    "df_final3['Noticias'] = df_final3['Noticias'].apply(lambda x: traduzir(x))\n",
    "\n",
    "#### Gerando features Roberta\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "def neu_rob(frase):\n",
    "    #trunca a frase para 514 caracteres (máximo suportado pelo modelo de Roberta)\n",
    "    frase = frase[:514]\n",
    "    \n",
    "    encoded_text = tokenizer(frase, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    neg_roberta = scores[0]\n",
    "   # neu_roberta = scores[1]\n",
    "   # pos_roberta = scores[2]\n",
    "    return neg_roberta\n",
    "\n",
    "df_final4 = df_final.copy()\n",
    "df_final4['Noticias'] = df_final3['Noticias']\n",
    "df_final4['neu_rob'] = df_final4['Noticias'].apply(lambda x: neu_rob(str(x)))\n",
    "\n",
    "\n",
    "#### Gerando features Finbert\n",
    "\n",
    "#pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "df_bert = df_final.copy()\n",
    "df_bert['Noticias'] = df_final3['Noticias']\n",
    "\n",
    "def sentimento_finbert_neg(string):\n",
    "    results = nlp([string])\n",
    "    dict_results = results[0]\n",
    "    sentimento = dict_results.get('label')\n",
    "    \n",
    "    if sentimento == \"Negative\":\n",
    "        score = -1*dict_results.get('score')\n",
    "    else:\n",
    "        score = 0\n",
    "        \n",
    "    return score\n",
    "\n",
    "df_bert['neg_finbert'] = df_bert['Noticias'].apply(lambda x: sentimento_finbert_neg(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando features de dias anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentilex + Vader\n",
    "df_final8 = df_final3.merge(df_final2, how = 'left', on = ['Date', 'Adj Close', 'Volume', 'Var%', 'Fechamento'])\n",
    "# + Roberta\n",
    "df_final8 = df_final8.merge(df_final4, how = 'left', on = ['Date', 'Adj Close', 'Volume', 'Var%', 'Fechamento'])\n",
    "# +Finbert\n",
    "df_final8 = df_final8.merge(df_bert, how = 'left', on = ['Date', 'Adj Close', 'Volume', 'Var%', 'Fechamento'])\n",
    "\n",
    "\n",
    "df_final99 = df_final8.copy()\n",
    "\n",
    "features = ['score', 'neu_rob','neg_finbert']\n",
    "featuresd1 = [i + \"d1\" for i in features]\n",
    "featuresd2 = [i + \"d2\" for i in features]\n",
    "featuresd3 = [i + \"d3\" for i in features]\n",
    "featuresd4 = [i + \"d4\" for i in features]\n",
    "\n",
    "#Criando as colunas de features para d-1, d-2, d-3, d-4 e inicializando com valores zeros:\n",
    "for i in features:\n",
    "    df_final99[i+\"d1\"] = 0\n",
    "    df_final99[i+\"d2\"] = 0\n",
    "    df_final99[i+\"d3\"] = 0\n",
    "    df_final99[i+\"d4\"] = 0\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "#atualiza as features de d-1\n",
    "for a,b in itertools.zip_longest(features,featuresd1):\n",
    "    df_final99[b] = df_final99.shift(periods=1)[a]\n",
    "    \n",
    "#atualiza as features de d-2\n",
    "for a,b in itertools.zip_longest(features,featuresd2):\n",
    "    df_final99[b] = df_final99.shift(periods=2)[a]\n",
    "    \n",
    "#atualiza as features de d-3\n",
    "for a,b in itertools.zip_longest(features,featuresd3):\n",
    "    df_final99[b] = df_final99.shift(periods=3)[a]\n",
    "    \n",
    "#atualiza as features de d-4\n",
    "for a,b in itertools.zip_longest(features,featuresd4):\n",
    "    df_final99[b] = df_final99.shift(periods=4)[a]\n",
    "\n",
    "\n",
    "df_final99 = df_final99.dropna()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GLQVL39etxA7",
    "w1XbpuI2txBD",
    "uVhYASlKtxBE",
    "kA8xUq80txBG",
    "WvunB2T2txBH",
    "yaZKt1aQtxBH",
    "o0FDRw9otxBI",
    "4Ps-TQJZtxBK",
    "Hvi7sE4mtxBK",
    "EnI0cgNDtxBL",
    "O2JRt_catxBL",
    "0t2vvLQstxBM",
    "WFxO_oNltxBN",
    "MR3swdq4txBO",
    "C6yWLR4StxBO",
    "BQF6Nd4NtxBW",
    "oqvY9hq6txBW",
    "O_VKfbbptxBX",
    "Q2aQq_0ZtxBY",
    "mmSDP734txBZ",
    "EVKPJo8KtxBZ",
    "gHYaLzzAtxBa",
    "u4_fMM9otxBa",
    "UMDvNtcitxBb",
    "TAmsdvUwtxBd",
    "mR1zw3eJtxBe",
    "F4t54GmWtxBf",
    "I-i7Chs1txBg",
    "DVLZJzx-txBh",
    "jSZ3ULpJtxBi",
    "c329l7BptxBj",
    "csFs7BZWtxBl",
    "NpEryjovtxBl",
    "UXo3Ahr0txBp",
    "aX2TRyThtxBr",
    "aIHcMoYXtxBt",
    "0bP1g5m0txBu",
    "7qGStQbAtxBw",
    "kI0uCb6ttxBz",
    "YvVsjbrptxB4",
    "AUrsC36KtxB-",
    "Gji2KE9ntxCB",
    "VIn0WCHTtxCC",
    "fZ134YHCtxCD",
    "d9p5fXTGtxCE",
    "MIeCms0NtxCF"
   ],
   "name": " Experimentos_+_EDA_(5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
